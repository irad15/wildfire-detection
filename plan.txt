# Wildfire Detection System

## Project Overview

Real-time wildfire detection system built with FastAPI that processes environmental sensor data (temperature, smoke, wind) to identify potential fire events using advanced signal processing and anomaly detection.

### Key Features
- Real-time REST API processing
- Savitzky-Golay signal smoothing (V1 baseline)
- Dynamic variance-aware anomaly detection
- Risk-based alerting (threshold: 70)
- Optional V2 path: spike suppression + alert hysteresis
- Benchmarked V1 vs V2
- Input validation with Pydantic models
- Centralized configuration management

## Architecture

```
wildfire-detection/
├── main.py                    # FastAPI application entry point
├── api/                       # API layer with validation
│   ├── __init__.py
│   ├── router.py              # REST API endpoints (/detect, /health)
│   └── validation.py          # Input validation (empty input check)
├── core/                      # Business logic layer
│   ├── __init__.py
│   ├── config.py              # Centralized configuration constants
│   ├── models.py              # Pydantic data models (DataPoint, Event, EventsSummary)
│   ├── data_processor.py      # Signal processing (V1 + V2 spike suppression)
│   ├── event_detector.py      # Anomaly detection (V1 + V2 hysteresis)
│   └── detection_service.py   # High-level orchestration service (uses V1 by default)
├── benchmarks/
│   └── benchmark_detection.py # Runtime + event-count benchmark (V1 vs V2)
├── data/                      # Sample input files for testing
│   ├── sample_input_1.json
│   ├── sample_input_1_spike.json
│   ├── sample_input_2.json
│   ├── sample_input_2_rise.json
│   ├── sample_input_flat.json
│   ├── sample_input_flat_rise.json
│   ├── sample_input_flat_rise_twice.json
│   └── sample_input_flat_spike.json
├── tests/                     # Comprehensive test suite
│   ├── api/
│   │   └── test_router.py     # API endpoint tests
│   └── core/
│       ├── test_data_processor.py  # Data processing unit tests
│       └── test_event_detector.py  # Event detection unit tests
│   
├── requirements.txt           # Python dependencies
├── README.md                  # Project documentation
└── plan.txt                   # This file
```

## API Endpoints

### POST /detect
**Input**: Array of `DataPoint` objects (timestamp, temperature, smoke, wind)  
**Output**: `EventsSummary` with events list, count, and max_score  
**Errors**: 422 for empty input or Pydantic validation failures

### GET /health
Returns `{"status": "ok"}`

## Processing Pipeline

### 1. Input Validation (API Layer)
- Checks for empty input (returns 422 if empty)
- Pydantic validates timestamp format (ISO-8601) and value ranges
- Early rejection prevents expensive processing on invalid inputs

### 2. Data Processing (DataProcessor)
- Sort chronologically by timestamp
- Extract signals to NumPy arrays
- V1: Apply Savitzky-Golay smoothing (window=13, polyorder=2) to temperature and smoke
- V2: Suppress isolated spikes (temp/smoke) before smoothing, then smooth
- Clip smoke to [0.0, 1.0] (physical bounds enforcement)
- Return smoothed DataPoint objects with original timestamp and wind preserved

### 3. Event Detection (EventDetector)
- Compute global statistics (mean, sample std with ddof=1)
- Apply dynamic damping via sigmoid functions:
  - Temperature: pivot=4.0°C, steepness=3.0
  - Smoke: pivot=0.02, steepness=20.0
- Convert z-scores to severity scores [0,1] using one-sided CDF
- Score wind via sigmoid (pivot=6.0 m/s, steepness=0.8)
- Calculate risk: `60 × temp_anomaly + 60 × smoke_anomaly + 15 × wind_score`
- V1: trigger events if score > 70
- V2: trigger once per incident with hysteresis reset at 65 to reduce duplicate alerts

## Algorithm Details

**Risk Score Formula**:
```
temp_z = (temp - mean_temp) / std_temp
smoke_z = (smoke - mean_smoke) / std_smoke
temp_severity = z_to_severity(temp_z)  # one-sided CDF
smoke_severity = z_to_severity(smoke_z)
temp_damping = sigmoid(std_temp, pivot=4.0, steepness=3.0)
smoke_damping = sigmoid(std_smoke, pivot=0.02, steepness=20.0)
wind_score = sigmoid(wind, pivot=6.0, steepness=0.8)
risk_score = 60×temp_severity×temp_damping + 60×smoke_severity×smoke_damping + 15×wind_score
risk_score = max(0.0, min(100.0, risk_score))
```

**Key Insights**:
- Variance-aware damping prevents false positives in stable conditions
- Severity mapping (one-sided CDF, not simple max(0,z)) provides bounded anomaly scores
- Multi-sensor fusion with balanced weights (60/60/15)
- Adaptive thresholds via sigmoid functions
- Configuration centralized in `core/config.py` for easy tuning

## Configuration

All tunable parameters are in `core/config.py`:
- **Data Processing**: `SAVITZKY_GOLAY_WINDOW`, `SAVITZKY_GOLAY_POLYORDER`
- **Spike Suppression (V2)**: `TEMP_SPIKE_THRESHOLD`, `SMOKE_SPIKE_THRESHOLD`
- **Event Detection**: `TEMP_WEIGHT`, `SMOKE_WEIGHT`, `WIND_BASE_WEIGHT`, `ALERT_THRESHOLD`
- **Damping Parameters**: `TEMP_PIVOT`, `TEMP_STEEPNESS`, `SMOKE_PIVOT`, `SMOKE_STEEPNESS`
- **Wind Scoring**: `WIND_PIVOT`, `WIND_STEEPNESS`
- **Hysteresis (V2)**: `HYSTERESIS_RESET_THRESHOLD`

## Usage

```bash
# Install dependencies
pip install -r requirements.txt

# Run service
uvicorn main:app --reload

# Test endpoint
curl -X POST "http://localhost:8000/detect" \
  -H "Content-Type: application/json" \
  -d @data/sample_input_1_spike.json

# Run tests
pytest
```

### Enable V2 pipeline locally (optional)
- In `core/detection_service.py`, switch to `process_v2` and `detect_v2`.
- Useful when benchmarking or evaluating spike-suppression + hysteresis behavior.

### Benchmarks
- Script: `python3 -m benchmarks.benchmark_detection`
- Compares V1 vs V2 runtime and event counts on `data/sample_input_flat_rise_twice.json`.

## Technical Stack

- **FastAPI**: REST API framework
- **Pydantic**: Data validation and models
- **NumPy/SciPy**: Signal processing (Savitzky-Golay filter)
- **pytest**: Testing framework
- **uvicorn**: ASGI server
